No global state, at least initially no closures, meaning it's easy to see where
mutability happens.  No functions as such, instead 'transformations', using two
argument lists, one for immutable indata, one for mutable outdata.

add :: (int, int) -> (int)
(a, b) add (c) {
  c = a + b
}

Out arguments are passed by reference, indata by unspecified means. I believe
this lack of specification should not change anything, as the data is
immutable.

Transforms can be overloaded based on all arguments, which allows convenient
things like writing an initialize function for all datatypes that may require
initialization, without requiring them all to have different names.

Providing two overloads where the only thing that differentiates them is the
type of number, for example float and int, should probably give at least
a warning, as providing a literal number might not give the intended effect.

It might be nice if transformations were flexible, in the way that if you have
an argument of a type, the transformation actually only requires the argument
to have those names with the correct types. Different versions of the
transformation would be generated for each of those, and it would provide some
sense of polymorphism, but with practically zero cost.

type Point struct {
  x, y int
}

type Square struct {
  x, y, width, height int
}

distance :: (Point, Point) -> (float)
(a, b) distance (dist) {
  (a*a + b*b) sqrt (dist)
}

distance would then for example work on both Point and Square. Disadvantages of
this method is that it can cause multiple overloads to be applicable to the
same argument lists. This should probably be at least a warning, quite possibly
an error. If this is used it should probably be possible to mark a transform as
strict, ie it requires the exact type, if it does something weird or just wants
some sanity. If the latter is a common reason the the default should perhaps be
the other way around, or the feature should be altogether dropped.

Custom allocators should be supported, but I don't really know all that much
about them, so I can't say very much. For now I'll just use malloc and friends.

I want no exceptions, error handling is go-like.

Declaring a local variable puts it on the stack. Assignment to a struct via '='
is either a shallow copy or disallowed, not sure which. Probably shallow copy,
keeping everything consistent. Possibly a warning for a shallow copy when the
struct contains an owned pointer.

varname Point // a stack-allocated (possibly optimized away) Point struct. Zero-initialized or not?
varname Point {
    x = 42
    y = 3
  } // same as before, but now with x initialized to 42 and y initialized to 3
varname Point {x = 42, y = 3} // same as above
varname Point = Point {x = 42, y = 3} // same as above
varname := Point {x = 42, y = 3} // inference perhaps?
varname Point {42, 3} // probably same as above
varname Point {x = 42} // leaves y uninitialized or zeroed
varname Point {42} // compile time error or same as above?
varname = Point {42, 3} // assigns the x and y in varname to those in the given struct. Actually shallow copy of rvalue

type Intlist struct {
  cap uint8
  !int elems
}

The ! is an example of possible syntax to denote an owned pointer. The only
thing it means is that the autogenerated dealloc/destroy will free the memory
to which it points. It could also generate extra warnings when you try to do
something weird, that may require annotations to silence to show that you've
actually thought it through.

Some transforms are autogenerated. Examples include dealloc/destroy and move.
Given Intlist above:

dealloc :: () -> (Intlist)
() dealloc (list) {
  () free (list.elems)
}

move :: () -> (Intlist, Intlist)
() move (from, to) {
  to.elems, from.elems = from.elems, null
  to.cap = from.cap
}

Syntax for freeing and allocating on heap is uncertain, it might be done as
automatically supplied transforms or as special syntax. Transforms would be
rather nice from a design viewpoint.

malloc, calloc, realloc and free would be typesafe automatically generated
transforms. If generics are allowed it might look like this:

malloc :: () -> (*t)
() malloc (p) {
  //actual malloc call like:
  //p = malloc(sizeof(t));
}
malloc :: (uint) -> (*t)
(count) malloc (p) {
  //actual malloc call like:
  //p = malloc(sizeof(t)*count);
}
calloc :: () -> (*t)
() calloc (p) {
  //actual calloc call like:
  //p = calloc(sizeof(t))
}
calloc :: (uint) -> (*t)
(count) calloc (p) {
  //actual calloc call like:
  //p = calloc(sizeof(t)*count);
}
realloc :: (uint) -> (*t)
(count) realloc (p) {
  //actual realloc call like:
  //p = realloc(p, sizeof(t)*count);
}

All of these may fail, but most of the time they work. It should probably be
fine to have these overloads that just explode if the returned pointer is null
(at least in debug), as well as another overload that additionally provides an
outargument for reporting the error.

It may or may not be a good idea to change the names, since it might confuse
people that the count parameter is kinda nice.

It might be nice to treat pointers to continuous chunks of memory as something
different from normal pointers. malloc will have to keep track of the size of
a memoryblock, meaning the extra information of how long the chunk is isn't
actually extra, it will just be stored twice until malloc is replaced (if it
is).

malloc (uint) -> ([]t)
(count) malloc (chunkPointer) {
  chunkPointer.cap = count
  //chunkPointer = malloc(sizeof(t)*count);
}

In that case pointer[2] would not be allowed, but chunkPointer[2] would.
*chunkPointer would probably not be allowed.

Having some sort way to use a function/transform in an expression position is
probably necessary for ease of use. It should only be doable when there is only
one outargument, otherwise it seems kinda hard to define. Additionally these
functions should probably be restricted to pure functions, and possibly even to
'simple' types, ints, floats, bools, etc.

It may seem a strange limitation that it should only work with simple types,
but it would take care of most important cases, but prevent things that may
hide some complexity or memory usage. Not sure about this.

It has become apparent that some nice way to provide a reference to something
internal to a datastructure would be nice. Overloading [] might be the way to
go, it would allow things like matrix[row, col] to be a reference to the
datacell in question. Alternatively transforms with a singe outargument of type
*t where t is some type might be allowed as lvalues, in which case things would
be written to the actual value. Example:

type Matrix<n> struct {
  cells ![]n
  ncols uint
}

read :: (Matrix<n>, uint, uint) -> (n)
(m, row, col) read (v) {
  v = m.cells[row * m.ncols + col]
}

read :: (Matrix<n>, uint, uint) -> (*n)
(m, row, col) read (p) {
  p = &m.cells[row * m.ncols + col]
}

// These two would then be equivalent
factor := 42
read(matrix, 3, 2) *= factor
read(matrix, 3, 2) = read(matrix, 3, 2) * factor

Alternatively we just realize that we can dereference the pointer to get
a usable lvalue (which then needs to be supported, but we already knew that) to
get what we want:

*read(matrix, 3, 2) *= factor

This of course assumes that the type system can select the correct variation of
read based on its 'return type'.

I think that a good first attempt at function overloading should be to have the
compiler emit an error when there are multiple overloads that fit a certain
use. This means it should generally be easy to figure out which one is used,
and also that the compiler never chooses the wrong one. It does however remove
the option of making a general version and then providing something more
specialized. I'm not entirely sure if a more specialized version is every
wanted, but if it is it might be a reasonable idea to provide a "specialized"
or "generalized" keyword, where the first means, "pick this version if multiple
exist" and the second means "don't pick this one in case of multiple fitting
versions". That particular thing will wait though.

Being able to generate a function in expression form, where the llvm ir
actually has it as a function with a return value, is probably a rather good
thing. It means that main can be generated without any kind of edge case. It
does however mean some changes need to be made in how things are generated,
which admittedly would need to be done anyway.

Function selection can probably only be done when all types are finalized,
which is only when the function has been requested. This means that the code
generator will have to do some more work here, to pick a fitting function
definition. In addition, types will have to be generated, assuming the current
version of type parameterization for structs will be used.

Things to generate:
- normal functions
- expression functions
- parameterized types

Normal types can just be straight up generated before all of that, and also
have a very simple way of choosing the name. Expression functions make some
instances of the function selection problem tricky. Consider:

(read(mat, row, col), true) write (list)

Functions involved:
- read :: (mat, row, col) -> (a)
- write :: (a, Bool) -> (list)

What function definitions are applicable depend on the definition chosen for
the other function. This also becomes an even bigger problem when there are
more expression functions involved.

Here the compiler would need to find all possible combinations and check if
they work. In most cases this shouldn't be too bad, but there are always worst
cases.

The finished compiler will probably have three or four stages:

- parsing, reads code into the AST. Should generate a 'basic' semantic version,
  not necessarily with a one to one mapping between language constructs and the
  AST. This is to simplify the later stages. This is also where int->i32 or i64
  should happen.

- type checking. Ensures that all names are known in the scope in which they
  are used, that all types match in expressions, propagates requirements to
  function signatures. Note that at this stage functions can only apply
  requirements in certain cases, if the known types are sufficient to select
  a singular function, which will probably not be done, at least not at first.

- propagate requirements through function call graph. Should be simple if it's
  acyclic, not so simple otherwise. Probably solvable, but is probably lower
  priority. The absence of this feature means that later function selection will
  have a few extra false negatives, where multiple functions will appear fit for
  selection when in reality many have requirements hidden in functions they call.

- code generation and function selection. What's currently implemented. Will
  have to be extended.

The list like structure that I had in mind has a fairly strong weakness: it only
stores the capacity, as that is the only thing that is pretty much guaranteed to be useful.
Many places where it would be used a length would however be useful, but I should
not force one to be present. This basically leads to either having two types,
which might be hard to differentiate syntactically, or just having one, requiring
the other to be implemented manually. The latter goes against the idea of this,
making it easy to write the least wasteful version, which basically says that it
should be trivial to change between one and the other.

[uint]*u8 is the original idea for syntax, where the type in [] denotes the specific
type storing the size of the thing. uint is default and can be omitted. One simple
way to differentiate would be to have some other delimiter, say () or {}. In that
case []*u8 would have length and capacity while ()*u8 would only have capacity, or
possibly the other way around. This would however be strange given that access will
probably be done exclusively using []. Switching would be different to distinguish
from expression functions as well as require a change in syntax when the semantics
change just a little bit.

Having different sizes of len and cap do not make sense to me,
otherwise the types could differ by one having a comma between the two
types, or just a comma if using the default size. Most of my ideas for
this so far would have some extra symbol within the [], which might
work, but it would look a bit strange I think.

Semantically a memorychunk/list should be equivalent with either
struct { cap : uint, content : *contenttype } or
* struct { cap : uint, content : [0]contenttype }

(This paragraph is going to talk about open and closed functions, it
refers to whether you can add alternate implementations or not) Having
all functions be 'open', meaning that a new definition from anywhere
can be considered when chosing functions is probably a bad idea, it'll
probably be way to hard to predict what is going to happen for most
things. Instead it might be better to have a function be closed by
default and then allowing it to be open by choice. Restricting each
open function to a single namespace should also help with making it
clear which one you're using. It does mean that you can't add support
for a new data type in a function if the function wasn't written to be
extensible, but that will probably end up being either a good thing or
at least a neutral thing.

This has a pretty high similarity to typeclasses for haskell, but
restricted to single functions. I am unsure if it is interesting to
just have typeclasses instead, or if this single function idea is
useful to have. Currently a namespace can look a lot like a typeclass,
except that there will be no requirement by the compiler to implement
all of the functions for a type.

More specifically most functions would be closed in the current
compilation unit while some are open. If a function is closed the only
definitions to be considered are in the current unit, otherwise they
might be elsewhere. Closed functions could therefore be fully selected
in type-checking. An open function has definitions from the original
unit, imported units, and the current unit.

# Attempt at type system

These are the only ones that exist during runtime. (concrete types)
- Simple types. i{8,16,32,64}, u{8,16,32,64}, f{32,64}, bool
- Compound types: struct, function/proc, array/memchunk, pointer

In a function signature more types may appear. Those are there to
enable a function to be generated for multiple concrete types, while
ensuring that no attempt will ever be made to generate an impossible
function. These other types are not actually types, rather
requirements and restrictions on types.

These include (a, b, c, etc represent concrete types, or type variables that may have requirements elsewhere):
- has{"prop" : a}, has a property .prop of type a
- num, is numerical, ie +-/*% works (not entirely sure about %, might not be
  applicable to non-integers). Not sure if this should include uints or not.
- float, is of a float type
- int, is of an int type (not uint)
- uint, is of a uint type
- typeclass instance. Not sure about name, but it would be nice with 2 or more parameters as well as -> rules

Rules: (=>) means 'can be narrowed to'
has{} xor (num or float or int or uint) xor bool
uint xor int xor float
num => uint or int or float
uint => u8 or u16...
int => i8 or i16...
float => f32 or f64
has{ps} => struct{ps'}, ps subset of ps'
has{len : a} => [a]b or [a,]b (without or with cap)
has{len : a, cap : a} => [a,]b

Typeclasses are trickier than the rest, they are kind of external to
the types. They would most likely be made to exist outside of the
type, on the function level, "this function requires there to be an
instance of these typeclasses with these typeparameters".  Note that
has[] is actually a typeclass, and should quite probably be
implemented as one.

// a is the container, b the indexing type, c the contained type
typeclass Accessor a -> b, c {
  accessor : (a, b) -> (^c)
}

I will probably postpone implementation of actual typeclasses, instead
leaving them for later. This means that for a while the [] operator
will not pass typechecking, even if it should be a correct usage of
it.

Note: the current implementation of type inference treats all named
types as different from the underlying type. There will be implicit
type elevation from a basic type to a named type when they are
unified, poisoning the basic value to always have the elevated type if
it can be changed. This will for example appear strange in a case like
this:

T : {a : int, b : int}

(int, int) -> ({a : int, b : int})
f(a, b)(ret) {
  ret.a = a
  ret.b = b
}

main ()() {
  t : T
  t = f(2, 4)
}

In main typeinference will deduced the type of f(2, 4) : {a : int, b :
int}, then unify with T, which works but changes the type of f(2, 4) :
T, which is different. Codegen may or may not deal with this in the
correct way, depending on how function implementation instantiation
works.

----------------------------------------------------------------------

I want to be able to differentiate the underlying location of data
from the means to access it, for example allowing SOA types, but also
containers that can be used in pleasant ways. The built-in list type
would for example be using this to build on top of a simple struct
with a pointer.

type Array<i, t> {
  hide ptr
  [i] | 0 <= i && i < len -> ptr[i]
  [s = 0 : e = len] | 0 <= s && s <= e && e <= len -> Slice { len = e - s, ptr = ^ptr[s] }
} { len : i, cap : i, ptr : ^t }

type Slice<i, t> {
  hide ptr
  [i] | 0 <= i && i < len -> ptr[i]
  [s = 0 : e = len] | 0 <= s && s <= e && e <= len -> Slice { len = e - s, ptr = ^ptr[s] }
} { len : i, ptr : ^t }

type Id<i> {
  value -> self
  next -> self + 1
  prev -> self - 1
} i

First, this idea assumes that each newtype introduces a special
'function' with the same name that converts from the underlying type
to the newtype, and that the syntax is a bit strange, not sure about
that. After that, it enables hygenic macros with very special forms,
namely identifiers or expressions within [], with some extra stuff.

The first curly braces represents alterations to the namespace of the
original type.

Type inference would look in the type for the existance of the given
name or []-expression, then pattern-match.

<pattern> [| <condition>] -> <replacement>

A <pattern> is either an identifier or brackets surrounding a list of
tokens. Each token is either an identifier (with optional default
value), a comma or a colon. Note that this precludes the usage of
colon anywhere in an expression to assert the type of something, which
is probably not what we want. It might be a good idea to have some
other token, as we want to be able to parse a []-expression without
knowing the type or having the types definition. The specific tokens
would be gathered after operators are finalized. They can probably be
rather few though. It is also possible that we want to allow any
operator that is not already reserved, which would actually be easy to
do, but would make any changes to existing operators kinda
breaking. Operators might be expected to remain mostly the same though
I guess.

A <condition> is something that will be inserted into the code as a
runtime check (which will throw an error with the location in the code
upon failure) right before the replacement is evaluated.

A <replacement> is an expression, meaning no side-effects and only
control flow if it calls a function. The replacement is done in a
hygienic fashion, no nameclashes etc., and each argument (in the case
of a []-expression) is only evaluated once. Name resolution here first
looks at arguments, then canonical form, then normal name
resolution. Note that the newtype exposed names are not available
here. A special name is availible to represent the current object of
canonical form, 'self'. An option here is also that the names of the
canonical form are not available directly, instead you have to access
them through self, but that might make things unnecessarily verbose.

These are less powerful than true macros (hygienic or otherwise), but
the type system knows about them and it should make it reasonably easy
to state where code should go. Nothing really fancy can happen here as
only functions are allowed, not procedures.

type Player {
  x -> transform.x
  y -> transform.y
  z -> transform.z
} { transform : ^Transform, alive : Bool }
type Player {
  import(transform) // Could get more than the above if a Transform has more than just x, y and z
} { transform : ^Transform, alive : Bool }

type SOAPlayers<i> {
  [i] -> SOAPlayer { array = ^self, index = i}
} { transforms : Array<i, Transform>, alives : Array<i, Bool> }
type SOAPlayer<i> {
  transform -> transforms[index]
  alives -> alives[index]
}

This allows SOA, but not in a very nice fashion. This particular
pattern could most definitely be automatically generated, but I'm not
sure how that mechanic should be implemented. It would be compile time
runnable, relatively fancy macro expansion. Maybe, but should probably
be saved for later.

------------------------

points where linebreaks do not break
- right after a prefix, before the expression
- right after a binop, before the second expression
- inside parenthesis -- TODO
- directly after "=" (in varinit and shallowcopy)
- after let, before expression
- after mut, before expression
- after :, in type assertion
- in type literal -- TODO
- after defer, before statement

------------------------

Needed from clang:
- how C does layout of structs (probably anyway)
- how a C function declaration translates to IR
  and what operations are needed to translate to/from it

interface reqs
- func to init clang
- func to destruct clang
- query func for struct layout
- query func for func layout
- query func to parse a header and retrieve all decls

------------------------

IRequirements, things that need to be represented:

structliteral with names: something with at least the specified (name, type) tuples
supports memberaccess (including not previously specified names)
is not numerical
is not a newtype

structliteral without names: something with exactly as many (name, type) tuples, but without information about name
supports memberaccess (exactly as many different as tuples, but that's probably overkill)
is not numerical
is not a newtype

unary negation requires numbers to be signed
bitwise ops require numbers to be integral (possibly unsigned)
float literals require numbers to be floating

a newtype on a numerical type inherits the same numerical-ness, thus a newtype must support a numerical requirement
note that this probably merits a way to hide the numerical-ness, to provide an opaque datatype that happens to be backed by a number
maybe just let "hide *" do this

{ mayBeNewType : Bool, numSpec : NumSpec, accesses : [MemberAccess], structTuple : Maybe [Type] }
NumSpec = NoSpec | NotNumerical | Signed | Floating | Integral | IntegralSigned | IntegralUnsigned
                                                                               -- not sure about this one

numerical -> not structliteral
structliteral -> not numerical, not newtype
any kind of literal -> not newtype

eventually these may need to express "can cast/convert from/to", unless a better mechanism is implmented for that

-------------------------

usability:
- nicer form of looping definitely required
- '+=' etc are surprisingly missed

alternative syntax for func and proc declarations:

proc # intypes # outtypes
name # innames # outnames {
  // do stuff
}

func(intypes) outtype
name(innames) outname {
  // do stuff
}

might also work and be nice as syntax for the respective types
(realized that proc # intypes # outtypes does not nest very well)


--------------------------

librustc_trans/trans/foreign.rs has trans_native_call, which I'll
probably want to steal for calling to C and stuff

it seems that the representation of a C struct is trivial in llvm, at
least I can't find anything that ensures it in any of the llvm based
compilers I have looked into so far.
